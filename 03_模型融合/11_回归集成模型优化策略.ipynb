{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "from manual_ensemble import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = [\n",
    "    'gender',\n",
    "    'SeniorCitizen',\n",
    "    'Partner',\n",
    "    'Dependents',\n",
    "    'PhoneService',\n",
    "    'MultipleLines',\n",
    "    'InternetService',\n",
    "    'OnlineSecurity',\n",
    "    'OnlineBackup',\n",
    "    'DeviceProtection',\n",
    "    'TechSupport',\n",
    "    'StreamingTV',\n",
    "    'StreamingMovies',\n",
    "    'Contract',\n",
    "    'PaperlessBilling',\n",
    "    'PaymentMethod',\n",
    "]\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges'] = (\n",
    "    tcc['TotalCharges'].apply(lambda x: x if x != ' ' else np.nan).astype(float)\n",
    ")\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化\n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No', value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month'] - 1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month'] - 1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(\n",
    "    enc.transform(X_train_seq).toarray(), columns=cate_colName(enc, seq_new, drop=None)\n",
    ")\n",
    "\n",
    "X_test_seq = pd.DataFrame(\n",
    "    enc.transform(X_test_seq).toarray(), columns=cate_colName(enc, seq_new, drop=None)\n",
    ")\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(\n",
    "    ord_enc.transform(X_train[category_cols]), columns=category_cols\n",
    ")\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(\n",
    "    ord_enc.transform(X_test[category_cols]), columns=category_cols\n",
    ")\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本节新增第三方库\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier_threshold(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, estimators, voting=\"hard\", weights=None, thr=0.5):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        self.thr = thr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        VC = VotingClassifier(\n",
    "            estimators=self.estimators, voting=self.voting, weights=self.weights\n",
    "        )\n",
    "\n",
    "        VC.fit(X, y)\n",
    "        self.clf = VC\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.voting == \"soft\":\n",
    "            res_proba = self.clf.predict_proba(X)\n",
    "        else:\n",
    "            res_proba = None\n",
    "        return res_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.voting == \"soft\":\n",
    "            res = (self.clf.predict_proba(X)[:, 1] >= self.thr) * 1\n",
    "        else:\n",
    "            res = self.clf.predict(X)\n",
    "        return res\n",
    "\n",
    "    def score(self, X, y):\n",
    "        acc = accuracy_score(self.predict(X), y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "\n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [\n",
    "    (X_train1, y_train1),\n",
    "    (X_train2, y_train2),\n",
    "    (X_train3, y_train3),\n",
    "    (X_train4, y_train4),\n",
    "    (X_train5, y_train5),\n",
    "]\n",
    "\n",
    "eval_set = [\n",
    "    (X_eval1, y_eval1),\n",
    "    (X_eval2, y_eval2),\n",
    "    (X_eval3, y_eval3),\n",
    "    (X_eval4, y_eval4),\n",
    "    (X_eval5, y_eval5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load(\"./model/grid_RF_1.joblib\")\n",
    "grid_RF_2 = load(\"./model/grid_RF_2.joblib\")\n",
    "grid_RF_3 = load(\"./model/grid_RF_3.joblib\")\n",
    "grid_RF_4 = load(\"./model/grid_RF_4.joblib\")\n",
    "grid_RF_5 = load(\"./model/grid_RF_5.joblib\")\n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load(\"./model/grid_tree_1.joblib\")\n",
    "grid_tree_2 = load(\"./model/grid_tree_2.joblib\")\n",
    "grid_tree_3 = load(\"./model/grid_tree_3.joblib\")\n",
    "grid_tree_4 = load(\"./model/grid_tree_4.joblib\")\n",
    "grid_tree_5 = load(\"./model/grid_tree_5.joblib\")\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load(\"./model/grid_lr_1.joblib\")\n",
    "grid_lr_2 = load(\"./model/grid_lr_2.joblib\")\n",
    "grid_lr_3 = load(\"./model/grid_lr_3.joblib\")\n",
    "grid_lr_4 = load(\"./model/grid_lr_4.joblib\")\n",
    "grid_lr_5 = load(\"./model/grid_lr_5.joblib\")\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(\n",
    "    RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_RF = pd.Series(\n",
    "    RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_RF = pd.Series(\n",
    "    RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_RF = pd.Series(\n",
    "    RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_RF = pd.Series(\n",
    "    RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_RF = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_RF,\n",
    "        eval2_predict_proba_RF,\n",
    "        eval3_predict_proba_RF,\n",
    "        eval4_predict_proba_RF,\n",
    "        eval5_predict_proba_RF,\n",
    "    ]\n",
    ").sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(\n",
    "    tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_tree = pd.Series(\n",
    "    tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_tree = pd.Series(\n",
    "    tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_tree = pd.Series(\n",
    "    tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_tree = pd.Series(\n",
    "    tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_tree = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_tree,\n",
    "        eval2_predict_proba_tree,\n",
    "        eval3_predict_proba_tree,\n",
    "        eval4_predict_proba_tree,\n",
    "        eval5_predict_proba_tree,\n",
    "    ]\n",
    ").sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(\n",
    "    lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_lr = pd.Series(\n",
    "    lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_lr = pd.Series(\n",
    "    lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_lr = pd.Series(\n",
    "    lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_lr = pd.Series(\n",
    "    lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_lr = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_lr,\n",
    "        eval2_predict_proba_lr,\n",
    "        eval3_predict_proba_lr,\n",
    "        eval4_predict_proba_lr,\n",
    "        eval5_predict_proba_lr,\n",
    "    ]\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_housing = fetch_california_housing()\n",
    "\n",
    "X = pd.DataFrame(cal_housing.data, columns=cal_housing.feature_names)\n",
    "\n",
    "y = cal_housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16512, 8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6134</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.560729</td>\n",
       "      <td>0.939271</td>\n",
       "      <td>1552.0</td>\n",
       "      <td>3.141700</td>\n",
       "      <td>37.93</td>\n",
       "      <td>-121.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.3578</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.455598</td>\n",
       "      <td>1.007722</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>4.131274</td>\n",
       "      <td>32.80</td>\n",
       "      <td>-117.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.5111</td>\n",
       "      <td>16.0</td>\n",
       "      <td>5.716747</td>\n",
       "      <td>1.037905</td>\n",
       "      <td>3903.0</td>\n",
       "      <td>2.689869</td>\n",
       "      <td>34.25</td>\n",
       "      <td>-118.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.1124</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.623188</td>\n",
       "      <td>1.019324</td>\n",
       "      <td>1153.0</td>\n",
       "      <td>2.785024</td>\n",
       "      <td>34.11</td>\n",
       "      <td>-118.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2957</td>\n",
       "      <td>25.0</td>\n",
       "      <td>6.627832</td>\n",
       "      <td>1.008091</td>\n",
       "      <td>2128.0</td>\n",
       "      <td>3.443366</td>\n",
       "      <td>37.25</td>\n",
       "      <td>-121.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  6.6134       4.0  6.560729   0.939271      1552.0  3.141700     37.93   \n",
       "1  2.3578      41.0  5.455598   1.007722      1070.0  4.131274     32.80   \n",
       "2  5.5111      16.0  5.716747   1.037905      3903.0  2.689869     34.25   \n",
       "3  8.1124      52.0  6.623188   1.019324      1153.0  2.785024     34.11   \n",
       "4  6.2957      25.0  6.627832   1.008091      2128.0  3.443366     37.25   \n",
       "\n",
       "   Longitude  \n",
       "0    -121.97  \n",
       "1    -117.15  \n",
       "2    -118.61  \n",
       "3    -118.14  \n",
       "4    -121.81  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of RandomForest:\n",
      "Train-MSE: 0.035168, Test-MSE: 0.250861\n",
      "The results of ExtraTrees:\n",
      "Train-MSE: 0.000000, Test-MSE: 0.243643\n",
      "The results of GBDT:\n",
      "Train-MSE: 0.260555, Test-MSE: 0.277794\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "reg1 = RandomForestRegressor(random_state=12).fit(X_train, y_train)\n",
    "print(\"The results of RandomForest:\")\n",
    "print(\n",
    "    \"Train-MSE: %f, Test-MSE: %f\"\n",
    "    % (\n",
    "        mean_squared_error(reg1.predict(X_train), y_train),\n",
    "        mean_squared_error(reg1.predict(X_test), y_test),\n",
    "    )\n",
    ")\n",
    "\n",
    "# 极端随机树\n",
    "reg2 = ExtraTreesRegressor(random_state=12).fit(X_train, y_train)\n",
    "print(\"The results of ExtraTrees:\")\n",
    "print(\n",
    "    \"Train-MSE: %f, Test-MSE: %f\"\n",
    "    % (\n",
    "        mean_squared_error(reg2.predict(X_train), y_train),\n",
    "        mean_squared_error(reg2.predict(X_test), y_test),\n",
    "    )\n",
    ")\n",
    "\n",
    "# GBDT\n",
    "reg3 = GradientBoostingRegressor(random_state=12).fit(X_train, y_train)\n",
    "print(\"The results of GBDT:\")\n",
    "print(\n",
    "    \"Train-MSE: %f, Test-MSE: %f\"\n",
    "    % (\n",
    "        mean_squared_error(reg3.predict(X_train), y_train),\n",
    "        mean_squared_error(reg3.predict(X_test), y_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of RandomForest:\n",
      "Train-cross-mean: 0.259973\n",
      "The results of ExtraTrees:\n",
      "Train-cross-mean: 0.258106\n",
      "The results of GBDT:\n",
      "Train-cross-mean: 0.287060\n"
     ]
    }
   ],
   "source": [
    "# 随机森林\n",
    "reg1 = RandomForestRegressor(random_state=12)\n",
    "print(\"The results of RandomForest:\")\n",
    "print(\n",
    "    \"Train-cross-mean: %f\"\n",
    "    % -(\n",
    "        cross_val_score(\n",
    "            reg1, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        )\n",
    "    ).mean()\n",
    ")\n",
    "\n",
    "# 极端随机树\n",
    "reg2 = ExtraTreesRegressor(random_state=12)\n",
    "print(\"The results of ExtraTrees:\")\n",
    "print(\n",
    "    \"Train-cross-mean: %f\"\n",
    "    % -(\n",
    "        cross_val_score(\n",
    "            reg2, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        )\n",
    "    ).mean()\n",
    ")\n",
    "\n",
    "# GBDT\n",
    "reg3 = GradientBoostingRegressor(random_state=12)\n",
    "print(\"The results of GBDT:\")\n",
    "print(\n",
    "    \"Train-cross-mean: %f\"\n",
    "    % -(\n",
    "        cross_val_score(\n",
    "            reg3, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        )\n",
    "    ).mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {\n",
    "    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 20, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 2, 20, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 50, 1),\n",
    "    \"max_leaf_nodes\": hp.quniform(\"max_leaf_nodes\", 50, 300, 1),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 50, 500, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 0.8),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "    min_samples_split = int(params[\"min_samples_split\"])\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_depth=max_depth,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_RF, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=15\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [1:04:14<00:00,  7.71s/trial, best loss: 0.28465633940450197]\n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 31.0,\n",
       " 'max_leaf_nodes': 299.0,\n",
       " 'max_samples': 0.7718695504904937,\n",
       " 'min_samples_leaf': 3.0,\n",
       " 'min_samples_split': 20.0,\n",
       " 'n_estimators': 307.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=31, max_leaf_nodes=299,\n",
       "                      max_samples=0.7718695504904937, min_samples_leaf=3,\n",
       "                      min_samples_split=20, n_estimators=307)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=31, max_leaf_nodes=299,\n",
       "                      max_samples=0.7718695504904937, min_samples_leaf=3,\n",
       "                      min_samples_split=20, n_estimators=307)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=31, max_leaf_nodes=299,\n",
       "                      max_samples=0.7718695504904937, min_samples_leaf=3,\n",
       "                      min_samples_split=20, n_estimators=307)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg = RF_param_objective(RF_best_param, train=False)\n",
    "RF_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19617896082126313, 0.2761455219853933)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg.predict(X_train), y_train), mean_squared_error(\n",
    "    RF_reg.predict(X_test), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 大范围超参数搜索优化（方案A）\n",
    "\n",
    "&emsp;&emsp;首先也是最容易想到的策略就是增加搜索的超参数，以此降低模型过拟合风险，并且同时增加各超参数搜索范围，以此提升模型泛化能力。这当然是理论上的最优方案，但实际执行难度较大，最核心的困难就在于算力不够。目前的数据集在当前搜索空间范围内迭代500次需要1小时时间，而如果增加超参数数量、并增加搜索空间，超参数优化计算所需时间将呈指数级上升。此外，对于部分模型的部分超参数甚至根本无法带入进行搜索，例如回归随机森林中的criterion参数，根据参数说明可知，当criterion取值为absolute_error时将极大程度影响计算效率、计算时间将大幅提升，这里我们可以简单进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7389304637908936\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "RandomForestRegressor().fit(X_train, y_train)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283.6328864097595\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "RandomForestRegressor(criterion=\"absolute_error\").fit(X_train, y_train)\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够发现计算时间增幅达50倍！当然这不仅仅是回归随机森林单独模型的问题，很多模型在处理回归问题时都有类似的超参数设置情况，稍后建模的回归GBDT模型更是有过之而无不及。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['auto',\n",
       " 'sqrt',\n",
       " 'log2',\n",
       " None,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新设计超参数搜索\n",
    "max_features_range = [\"auto\", \"sqrt\", \"log2\", None] + np.arange(0.1, 1., 0.1).tolist()\n",
    "max_features_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 200, 800, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 1),\n",
    "    \"min_samples_leaf\": hp.quniform(\"min_samples_leaf\", 1, 50, 1),\n",
    "    \"min_samples_split\": hp.quniform(\"min_samples_split\", 2, 50, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 20, 120, 1),\n",
    "    \"max_leaf_nodes\": hp.quniform(\"max_leaf_nodes\", 200, 600, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    min_samples_leaf = int(params[\"min_samples_leaf\"])\n",
    "    min_samples_split = int(params[\"min_samples_split\"])\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        min_samples_split=min_samples_split,\n",
    "        max_depth=max_depth,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_RF, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [2:14:50<00:00,  8.09s/trial, best loss: 0.2572447927363112]  \n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_A = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13417130464725555, 0.25010856874888276)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_A.predict(X_train), y_train), mean_squared_error(\n",
    "    RF_reg_A.predict(X_test), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 更高效的搜索流程（方案B）\n",
    "\n",
    "&emsp;&emsp;这里我们考虑另一种更高效的搜索流程：直接舍弃基础学习器的超参数搜索，只搜索集成相关超参数。这里需要注意的是，舍弃基础学习器的超参数搜索其实就等于带入基础学习器的默认超参数组，而基础学习器的默认超参数组是不会剪枝的、即默认执行最高模型复杂度的模型训练，每个基础学习器实际上是容易过拟合的，但是考虑到回归问题本身就要求更复杂的模型进行训练，且已被上述搜索过程验证，因此基础学习器选取默认超参数并不会对预测结果有较大的影响。并且除了基础学习器的超参数外、对模型泛化能力影响更大的继承超参数仍然是朝着泛化能力更强的方向进行搜索，外加舍弃了这部分超参数搜索，整体超参数搜索的速度将变得更快，这将有助于在更短的时间内快速搜索出一组更好的结果。        \n",
    "&emsp;&emsp;但是需要注意，如果接下来采用方案二进行超参数搜索，当我们舍弃了更多的基础学习器的超参数的时候，最终训练集的预测结果将会有更大的过拟合风险，因此在方案二的超参数搜索时，仍然需要坚持使用交叉验证结果作为模型泛化能力的评分标准。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 20, 700, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_RF, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=15\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:10<00:00,  8.60s/trial, best loss: 0.2427352199984794] \n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 2, 'max_samples': 0.9297616244399035, 'n_estimators': 626.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_B1 = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03702785158995616, 0.23246909513790037)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_B1.predict(X_train), y_train), mean_squared_error(\n",
    "    RF_reg_B1.predict(X_test), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 训练方式 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| 默认超参数 | <center>0.0351 |<center>0.2599 | <center>0.2508 |\n",
    "| 小范围搜索 | <center>0.1943 |<center>0.2850 | <center>0.2760 |\n",
    "| 大范围搜索（A） | <center>0.1328 |<center>0.2578 | <center>0.2500 |\n",
    "| 高效搜索（B） | <center>0.0321 |<center>0.2414 | <center>0.2305 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，此时由于诸多基础学习器的超参数未经过优化，因此在训练集上的过拟合风险加剧，但交叉验证的平均得分和测试集评分接近，说明交叉验证的平均得分仍然能够非常好的衡量模型泛化能力，说明模型搜索过程仍然有效。当然，通过和此前结果对比我们不难判断，训练集上的过拟合问题确实是由于超参数设置和搜索不完全导致的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相比通过限制搜索空间来提高搜索效率，有的时候适度的放开模型的学习能力，哪怕让模型学到了一些局部规律，只要不严重影响测试集预测结果、不影响模型整体泛化能力，其实也没有太大问题，例如模型4。对于4号模型来说，由于不对基础模型的超参数进行限制，因此其学习能力是非常强的，但由于我们对其集成超参数进行了搜索和优化，因此仍然一定程度上对其学习能力进行了调整，在有限的调整空间下，我们无法获得类似模型3中获得的严谨结果，但仍然保证了其能够有效学习全局规律，因此最终预测集上结果有所提升。但由于还有很多不受控的超参数在影响模型的学习能力，因此模型在训练集的训练过程中也学习了非常多局部规律，导致其但从训练集上来看过拟合风险偏高。但就模型泛化能力、也就是对新数据的预测能力来说，模型4和模型3并无本质区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先我们需要确定一点的是，从理论层面来说，超参数搜索最有效的方法一定是全参数大范围高精度搜索，如此得到的模型一定是能够最大程度学习全局规律的模型。但遗憾的是，在实际执行过程中，尤其是回归类问题，受限于当前算力条件，该方案几乎没有可执行性。因此需要考虑挑选出部分超参数进行搜索，而只要是挑选部分超参数出来进行搜索，其实就已经是不满足理论最优解的情况了，而具体要选哪些超参数出来搜索，也不存在理论最优解。因此，关于超参数的选取，其实也是需要多次尝试并且不断积累经验的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;总的来说，根据长期实践经验总结，集成相关超参数肯定需要进行搜索的，例如随机森林中的'max_features'、'n_estimators'和'max_samples'等，此外对于基础学习器的超参数来说，可以优先尝试max_depth、max_leaf_nodes这两个参数，该参数对决策树模型剪枝影响巨大。当然，是否要加入max_depth、max_leaf_nodes，甚至是其他基础分类器超参数进行搜索，可以通过少量次数迭代测试效果再决定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:52<00:00,  9.45s/trial, best loss: 0.2419435578184832] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 64.0,\n",
       " 'max_features': 7,\n",
       " 'max_samples': 0.996508030907065,\n",
       " 'n_estimators': 423.0}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_space = {\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 200, 800, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 20, 120, 1),\n",
    "}\n",
    "\n",
    "\n",
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_RF, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )\n",
    "\n",
    "\n",
    "RF_param_search(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比不带入max_depth情况交叉验证平均得分更差，因此这里不宜带入max_depth进行搜索。接下来继续考虑带入max_leaf_nodes的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:07<00:00,  7.34s/trial, best loss: 0.26178265241806653]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 110.0,\n",
       " 'max_features': 7,\n",
       " 'max_leaf_nodes': 532.0,\n",
       " 'max_samples': 0.9958345197920873,\n",
       " 'n_estimators': 794.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_space = {\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 200, 800, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 1),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 20, 120, 1),\n",
    "    \"max_leaf_nodes\": hp.quniform(\"max_leaf_nodes\", 200, 600, 1),\n",
    "}\n",
    "\n",
    "\n",
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    max_leaf_nodes = int(params[\"max_leaf_nodes\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        max_depth=max_depth,\n",
    "        max_leaf_nodes=max_leaf_nodes,\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_RF, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )\n",
    "\n",
    "\n",
    "RF_param_search(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当然，既然是诞生于实践过程的搜索策略，往往就会有长期实践过程中逐渐摸索出来的优化流程。对于这里我们看到的基于集成参数的高效搜索流程也不例外。整体来说，由于4号模型经过优化的超参数个数较少，因此模型在不同数据集上进行训练和预测时偏差较小但方差较大（例如max_depth取值为None时，就会根据不同数据集情况训练得到不同深度的基础树模型），目前来说，较为通用的围绕高效搜索流程算法特性设计的优化方法有以下两种，这两种方法的基本思路都源于交叉训练，希望通过类似交叉训练的过程来降低输出结果方差，提升结果稳定性，进而提高模型泛化能力："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法一：在搜索得到一组最优超参数后，通过交叉训练的方式同时输出多组测试集的预测结果，然后求均值作为最终的预测结果；      \n",
    "方法二：在超参数搜索过程中，不再使用验证集的平均得分作为搜索依据，换成验证集拼接而成的预测数据和标签之间的MSE作为搜索依据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_features=&#x27;log2&#x27;, max_samples=0.9297616244399035,\n",
       "                      n_estimators=626, n_jobs=-1, random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=&#x27;log2&#x27;, max_samples=0.9297616244399035,\n",
       "                      n_estimators=626, n_jobs=-1, random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_features='log2', max_samples=0.9297616244399035,\n",
       "                      n_estimators=626, n_jobs=-1, random_state=12)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg_B1.set_params(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方案一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_test_predict = 0\n",
    "RF_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    RF_reg_B1.fit(X_train_part, y_train_part)\n",
    "    RF_train_predict += RF_reg_B1.predict(X_train) / 5\n",
    "    RF_test_predict += RF_reg_B1.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06412037340179796"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_train_predict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23652047927469372"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03702785158995616, 0.23246909513790034)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_reg_B1.fit(X_train, y_train)\n",
    "mean_squared_error(RF_reg_B1.predict(X_train), y_train), mean_squared_error(\n",
    "    RF_reg_B1.predict(X_test), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方案二"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_space = {\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 20, 700, 1),\n",
    "    \"max_samples\": hp.uniform(\"max_samples\", 0.2, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    max_samples = params[\"max_samples\"]\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_RF = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_samples=max_samples,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "        n_jobs=15,\n",
    "    )\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "\n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_RF.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_RF.predict(X_eval_part)\n",
    "\n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "\n",
    "    else:\n",
    "        res = reg_RF.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        RF_param_objective, space=RF_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:06<00:00,  3.74s/trial, best loss: 0.2442098878390925] \n"
     ]
    }
   ],
   "source": [
    "RF_best_param = RF_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 2, 'max_samples': 0.9999921130888294, 'n_estimators': 189.0}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_reg_B2 = RF_param_objective(RF_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.032821170137090024, 0.2327842107292657)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(RF_reg_B2.predict(X_train), y_train), mean_squared_error(\n",
    "    RF_reg_B2.predict(X_test), y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归极端随机树超参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_space = {\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 50, 1),\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 20, 700, 1),\n",
    "    \"max_features\": hp.choice(\"max_features\", max_features_range),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_ET = ExtraTreesRegressor(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_ET, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=15\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_ET.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        ET_param_objective, space=ET_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [05:04<00:00,  6.09s/trial, best loss: 0.23461629604873618]\n"
     ]
    }
   ],
   "source": [
    "ET_best_param = ET_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 41.0, 'max_features': 9, 'n_estimators': 584.0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584,\n",
       "                    random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584,\n",
       "                    random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584,\n",
       "                    random_state=12)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_reg = ET_param_objective(ET_best_param, train=False)\n",
    "ET_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22305638031008407"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_reg.predict(X_train), y_train),\n",
    "mean_squared_error(ET_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584, n_jobs=-1,\n",
       "                    random_state=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ExtraTreesRegressor</label><div class=\"sk-toggleable__content\"><pre>ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584, n_jobs=-1,\n",
       "                    random_state=12)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "ExtraTreesRegressor(max_depth=41, max_features=0.6, n_estimators=584, n_jobs=-1,\n",
       "                    random_state=12)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ET_reg.set_params(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_test_predict = 0\n",
    "ET_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    ET_reg.fit(X_train_part, y_train_part)\n",
    "    ET_train_predict += ET_reg.predict(X_train) / 5\n",
    "    ET_test_predict += ET_reg.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2252541561394263"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_train_predict, y_train),\n",
    "mean_squared_error(ET_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ET_param_objective(params, train=True):\n",
    "    # 超参数读取\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "\n",
    "    if train == True:\n",
    "        max_features = params[\"max_features\"]\n",
    "\n",
    "    else:\n",
    "        max_features = max_features_range[params[\"max_features\"]]\n",
    "\n",
    "    # 模型创建\n",
    "    reg_ET = ExtraTreesRegressor(\n",
    "        max_depth=max_depth,\n",
    "        n_estimators=n_estimators,\n",
    "        max_features=max_features,\n",
    "        random_state=12,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "\n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_ET.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_ET.predict(X_eval_part)\n",
    "\n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    else:\n",
    "        res = reg_ET.fit(X_train, y_train)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:06<00:00,  2.53s/trial, best loss: 0.23324374995749944]\n"
     ]
    }
   ],
   "source": [
    "ET_best_param = ET_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ET_reg2 = ET_param_objective(ET_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22320853438271698"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ET_reg2.predict(X_train), y_train),\n",
    "mean_squared_error(ET_reg2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归GBDT超参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_space = {\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 20, 701, 1),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", 0.02, 0.2),\n",
    "    \"subsample\": hp.uniform(\"subsample\", 0.1, 1.0),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 2, 20, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "\n",
    "    reg_GBR = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        random_state=12,\n",
    "    )\n",
    "    if train == True:\n",
    "        res = -cross_val_score(\n",
    "            reg_GBR, X_train, y_train, scoring=\"neg_mean_squared_error\", n_jobs=-1\n",
    "        ).mean()\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_search(max_evals=500):\n",
    "    return fmin(\n",
    "        GBR_param_objective, space=GBR_space, algo=tpe.suggest, max_evals=max_evals\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [09:29<00:00, 11.40s/trial, best loss: 0.20933920422665336]\n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.02518721022024758,\n",
       " 'max_depth': 8.0,\n",
       " 'n_estimators': 550.0,\n",
       " 'subsample': 0.7943547875565606}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_reg = GBR_param_objective(GBR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.02518721022024758, max_depth=8,\n",
       "                          n_estimators=550, random_state=12,\n",
       "                          subsample=0.7943547875565606)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.02518721022024758, max_depth=8,\n",
       "                          n_estimators=550, random_state=12,\n",
       "                          subsample=0.7943547875565606)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.02518721022024758, max_depth=8,\n",
       "                          n_estimators=550, random_state=12,\n",
       "                          subsample=0.7943547875565606)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20238422969052877"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg.predict(X_train), y_train),\n",
    "mean_squared_error(GBR_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_test_predict = 0\n",
    "GBR_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    GBR_reg.fit(X_train_part, y_train_part)\n",
    "    GBR_train_predict += GBR_reg.predict(X_train) / 5\n",
    "    GBR_test_predict += GBR_reg.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19973202850449978"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_train_predict, y_train),\n",
    "mean_squared_error(GBR_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "\n",
    "    reg_GBR = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "\n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_GBR.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_GBR.predict(X_eval_part)\n",
    "\n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [49:30<00:00, 59.42s/trial, best loss: 0.21198462655059314]  \n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.07858979613062272,\n",
       " 'max_depth': 8.0,\n",
       " 'n_estimators': 285.0,\n",
       " 'subsample': 0.8397991762994271}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.07858979613062272, max_depth=8,\n",
       "                          n_estimators=285, random_state=12,\n",
       "                          subsample=0.8397991762994271)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.07858979613062272, max_depth=8,\n",
       "                          n_estimators=285, random_state=12,\n",
       "                          subsample=0.8397991762994271)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.07858979613062272, max_depth=8,\n",
       "                          n_estimators=285, random_state=12,\n",
       "                          subsample=0.8397991762994271)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_reg = GBR_param_objective(GBR_best_param, train=False)\n",
    "GBR_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2034893329166238"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg.predict(X_train), y_train), \n",
    "mean_squared_error(GBR_reg.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_test_predict = 0\n",
    "GBR_train_predict = 0\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "    # 在训练集上训练\n",
    "    X_train_part = X_train.loc[train_part_index]\n",
    "    y_train_part = y_train[train_part_index]\n",
    "    GBR_reg.fit(X_train_part, y_train_part)\n",
    "    GBR_train_predict += GBR_reg.predict(X_train) / 5\n",
    "    GBR_test_predict += GBR_reg.predict(X_test) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19705372635366933"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_train_predict, y_train),\n",
    "mean_squared_error(GBR_test_predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GBR_param_objective(params, train=True):\n",
    "    n_estimators = int(params[\"n_estimators\"])\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    subsample = params[\"subsample\"]\n",
    "    max_depth = int(params[\"max_depth\"])\n",
    "\n",
    "    reg_GBR = GradientBoostingRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        max_depth=max_depth,\n",
    "        random_state=12,\n",
    "    )\n",
    "\n",
    "    oof_series = pd.Series(np.empty(X_train.shape[0]))\n",
    "\n",
    "    if train == True:\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=12)\n",
    "        for train_part_index, eval_index in kf.split(X_train, y_train):\n",
    "            # 在训练集上训练\n",
    "            X_train_part = X_train.loc[train_part_index]\n",
    "            y_train_part = y_train[train_part_index]\n",
    "            reg_GBR.fit(X_train_part, y_train_part)\n",
    "            X_eval_part = X_train.loc[eval_index]\n",
    "            # 将验证集上预测结果拼接入oof数据集\n",
    "            oof_series.loc[eval_index] = reg_GBR.predict(X_eval_part)\n",
    "\n",
    "        res = mean_squared_error(oof_series, y_train)\n",
    "    else:\n",
    "        res = reg_GBR.fit(X_train, y_train)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [45:03<00:00, 54.08s/trial, best loss: 0.21141578335249117] \n"
     ]
    }
   ],
   "source": [
    "GBR_best_param = GBR_param_search(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.06061896900844315,\n",
       " 'max_depth': 8.0,\n",
       " 'n_estimators': 544.0,\n",
       " 'subsample': 0.5539249758481197}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBR_reg2 = GBR_param_objective(GBR_best_param, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(learning_rate=0.06061896900844315, max_depth=8,\n",
       "                          n_estimators=544, random_state=12,\n",
       "                          subsample=0.5539249758481197)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(learning_rate=0.06061896900844315, max_depth=8,\n",
       "                          n_estimators=544, random_state=12,\n",
       "                          subsample=0.5539249758481197)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=0.06061896900844315, max_depth=8,\n",
       "                          n_estimators=544, random_state=12,\n",
       "                          subsample=0.5539249758481197)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBR_reg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2048150038905001"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(GBR_reg2.predict(X_train), y_train),\n",
    "mean_squared_error(GBR_reg2.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 模型 | 训练集得分 | 交叉验证得分 | 测试集得分 |\n",
    "| ------ | ------ | ------ | ------ |\n",
    "| <center>随机森林 | <center>0.0351 |<center> 0.2599 | <center>0.2508 |\n",
    "| <center>随机森林_OPT | <center>0.0321 |<center>0.2414 | <center>0.2305 |\n",
    "| <center>极端随机树 | <center>0 |<center> 0.2581 | <center>0.2436 |\n",
    "| <center>极端随机树_OPT | <center>1.36e-07 |<center> 0.2346 | <center>0.2257 |\n",
    "| <center>GBDT | <center>0.2605 |<center> 0.2870 | <center>0.2777 |\n",
    "| <center>GBDT_OPT | <center>0.03545 |<center> 0.2091 | <center>0.1916 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model/GBR_reg.joblib']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(RF_reg, \"./model/RF_reg_B1.joblib\")\n",
    "dump(ET_reg, \"./model/ET_reg.joblib\")\n",
    "dump(GBR_reg, \"./model/GBR_reg.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
