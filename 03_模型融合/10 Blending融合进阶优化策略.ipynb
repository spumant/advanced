{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基础数据科学运算库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 可视化库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 时间模块\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# sklearn库\n",
    "# 数据预处理\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# 实用函数\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 常用评估器\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 网格搜索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 自定义评估器支持模块\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "\n",
    "# 自定义模块\n",
    "from telcoFunc import *\n",
    "from manual_ensemble import *\n",
    "\n",
    "# 导入特征衍生模块\n",
    "import features_creation as fc\n",
    "from features_creation import *\n",
    "\n",
    "# re模块相关\n",
    "import inspect, re\n",
    "\n",
    "# 其他模块\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "tcc = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# 标注连续/离散字段\n",
    "# 离散字段\n",
    "category_cols = [\n",
    "    'gender',\n",
    "    'SeniorCitizen',\n",
    "    'Partner',\n",
    "    'Dependents',\n",
    "    'PhoneService',\n",
    "    'MultipleLines',\n",
    "    'InternetService',\n",
    "    'OnlineSecurity',\n",
    "    'OnlineBackup',\n",
    "    'DeviceProtection',\n",
    "    'TechSupport',\n",
    "    'StreamingTV',\n",
    "    'StreamingMovies',\n",
    "    'Contract',\n",
    "    'PaperlessBilling',\n",
    "    'PaymentMethod',\n",
    "]\n",
    "\n",
    "# 连续字段\n",
    "numeric_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# 标签\n",
    "target = 'Churn'\n",
    "\n",
    "# ID列\n",
    "ID_col = 'customerID'\n",
    "\n",
    "# 验证是否划分能完全\n",
    "assert len(category_cols) + len(numeric_cols) + 2 == tcc.shape[1]\n",
    "\n",
    "# 连续字段转化\n",
    "tcc['TotalCharges'] = (\n",
    "    tcc['TotalCharges'].apply(lambda x: x if x != ' ' else np.nan).astype(float)\n",
    ")\n",
    "tcc['MonthlyCharges'] = tcc['MonthlyCharges'].astype(float)\n",
    "\n",
    "# 缺失值填补\n",
    "tcc['TotalCharges'] = tcc['TotalCharges'].fillna(0)\n",
    "\n",
    "# 标签值手动转化\n",
    "tcc['Churn'].replace(to_replace='Yes', value=1, inplace=True)\n",
    "tcc['Churn'].replace(to_replace='No', value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tcc.drop(columns=[ID_col, target]).copy()\n",
    "labels = tcc['Churn'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "train, test = train_test_split(tcc, random_state=22)\n",
    "\n",
    "X_train = train.drop(columns=[ID_col, target]).copy()\n",
    "X_test = test.drop(columns=[ID_col, target]).copy()\n",
    "\n",
    "y_train = train['Churn'].copy()\n",
    "y_test = test['Churn'].copy()\n",
    "\n",
    "X_train_seq = pd.DataFrame()\n",
    "X_test_seq = pd.DataFrame()\n",
    "\n",
    "# 年份衍生\n",
    "X_train_seq['tenure_year'] = ((72 - X_train['tenure']) // 12) + 2014\n",
    "X_test_seq['tenure_year'] = ((72 - X_test['tenure']) // 12) + 2014\n",
    "\n",
    "# 月份衍生\n",
    "X_train_seq['tenure_month'] = (72 - X_train['tenure']) % 12 + 1\n",
    "X_test_seq['tenure_month'] = (72 - X_test['tenure']) % 12 + 1\n",
    "\n",
    "# 季度衍生\n",
    "X_train_seq['tenure_quarter'] = ((X_train_seq['tenure_month'] - 1) // 3) + 1\n",
    "X_test_seq['tenure_quarter'] = ((X_test_seq['tenure_month'] - 1) // 3) + 1\n",
    "\n",
    "# 独热编码\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit(X_train_seq)\n",
    "\n",
    "seq_new = list(X_train_seq.columns)\n",
    "\n",
    "# 创建带有列名称的独热编码之后的df\n",
    "X_train_seq = pd.DataFrame(\n",
    "    enc.transform(X_train_seq).toarray(), columns=cate_colName(enc, seq_new, drop=None)\n",
    ")\n",
    "\n",
    "X_test_seq = pd.DataFrame(\n",
    "    enc.transform(X_test_seq).toarray(), columns=cate_colName(enc, seq_new, drop=None)\n",
    ")\n",
    "\n",
    "# 调整index\n",
    "X_train_seq.index = X_train.index\n",
    "X_test_seq.index = X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_enc = OrdinalEncoder()\n",
    "ord_enc.fit(X_train[category_cols])\n",
    "\n",
    "X_train_OE = pd.DataFrame(\n",
    "    ord_enc.transform(X_train[category_cols]), columns=category_cols\n",
    ")\n",
    "X_train_OE.index = X_train.index\n",
    "X_train_OE = pd.concat([X_train_OE, X_train[numeric_cols]], axis=1)\n",
    "\n",
    "X_test_OE = pd.DataFrame(\n",
    "    ord_enc.transform(X_test[category_cols]), columns=category_cols\n",
    ")\n",
    "X_test_OE.index = X_test.index\n",
    "X_test_OE = pd.concat([X_test_OE, X_test[numeric_cols]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本节新增第三方库\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from hyperopt import hp, fmin, tpe\n",
    "from numpy.random import RandomState\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VotingClassifier_threshold(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    def __init__(self, estimators, voting=\"hard\", weights=None, thr=0.5):\n",
    "        self.estimators = estimators\n",
    "        self.voting = voting\n",
    "        self.weights = weights\n",
    "        self.thr = thr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        VC = VotingClassifier(\n",
    "            estimators=self.estimators, voting=self.voting, weights=self.weights\n",
    "        )\n",
    "\n",
    "        VC.fit(X, y)\n",
    "        self.clf = VC\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.voting == \"soft\":\n",
    "            res_proba = self.clf.predict_proba(X)\n",
    "        else:\n",
    "            res_proba = None\n",
    "        return res_proba\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.voting == \"soft\":\n",
    "            res = (self.clf.predict_proba(X)[:, 1] >= self.thr) * 1\n",
    "        else:\n",
    "            res = self.clf.predict(X)\n",
    "        return res\n",
    "\n",
    "    def score(self, X, y):\n",
    "        acc = accuracy_score(self.predict(X), y)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化KFold评估器\n",
    "kf = KFold(n_splits=5, random_state=12, shuffle=True)\n",
    "\n",
    "# 重置训练集和测试集的index\n",
    "X_train_OE = X_train_OE.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "train_part_index_l = []\n",
    "eval_index_l = []\n",
    "\n",
    "for train_part_index, eval_index in kf.split(X_train_OE, y_train):\n",
    "    train_part_index_l.append(train_part_index)\n",
    "    eval_index_l.append(eval_index)\n",
    "\n",
    "# 训练集特征\n",
    "X_train1 = X_train_OE.loc[train_part_index_l[0]]\n",
    "X_train2 = X_train_OE.loc[train_part_index_l[1]]\n",
    "X_train3 = X_train_OE.loc[train_part_index_l[2]]\n",
    "X_train4 = X_train_OE.loc[train_part_index_l[3]]\n",
    "X_train5 = X_train_OE.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集特征\n",
    "X_eval1 = X_train_OE.loc[eval_index_l[0]]\n",
    "X_eval2 = X_train_OE.loc[eval_index_l[1]]\n",
    "X_eval3 = X_train_OE.loc[eval_index_l[2]]\n",
    "X_eval4 = X_train_OE.loc[eval_index_l[3]]\n",
    "X_eval5 = X_train_OE.loc[eval_index_l[4]]\n",
    "\n",
    "# 训练集标签\n",
    "y_train1 = y_train.loc[train_part_index_l[0]]\n",
    "y_train2 = y_train.loc[train_part_index_l[1]]\n",
    "y_train3 = y_train.loc[train_part_index_l[2]]\n",
    "y_train4 = y_train.loc[train_part_index_l[3]]\n",
    "y_train5 = y_train.loc[train_part_index_l[4]]\n",
    "\n",
    "# 验证集标签\n",
    "y_eval1 = y_train.loc[eval_index_l[0]]\n",
    "y_eval2 = y_train.loc[eval_index_l[1]]\n",
    "y_eval3 = y_train.loc[eval_index_l[2]]\n",
    "y_eval4 = y_train.loc[eval_index_l[3]]\n",
    "y_eval5 = y_train.loc[eval_index_l[4]]\n",
    "\n",
    "train_set = [\n",
    "    (X_train1, y_train1),\n",
    "    (X_train2, y_train2),\n",
    "    (X_train3, y_train3),\n",
    "    (X_train4, y_train4),\n",
    "    (X_train5, y_train5),\n",
    "]\n",
    "\n",
    "eval_set = [\n",
    "    (X_eval1, y_eval1),\n",
    "    (X_eval2, y_eval2),\n",
    "    (X_eval3, y_eval3),\n",
    "    (X_eval4, y_eval4),\n",
    "    (X_eval5, y_eval5),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机森林模型组\n",
    "grid_RF_1 = load(\"./model/grid_RF_1.joblib\")\n",
    "grid_RF_2 = load(\"./model/grid_RF_2.joblib\")\n",
    "grid_RF_3 = load(\"./model/grid_RF_3.joblib\")\n",
    "grid_RF_4 = load(\"./model/grid_RF_4.joblib\")\n",
    "grid_RF_5 = load(\"./model/grid_RF_5.joblib\")\n",
    "\n",
    "RF_1 = grid_RF_1.best_estimator_\n",
    "RF_2 = grid_RF_2.best_estimator_\n",
    "RF_3 = grid_RF_3.best_estimator_\n",
    "RF_4 = grid_RF_4.best_estimator_\n",
    "RF_5 = grid_RF_5.best_estimator_\n",
    "\n",
    "RF_l = [RF_1, RF_2, RF_3, RF_4, RF_5]\n",
    "\n",
    "# 决策树模型组\n",
    "grid_tree_1 = load(\"./model/grid_tree_1.joblib\")\n",
    "grid_tree_2 = load(\"./model/grid_tree_2.joblib\")\n",
    "grid_tree_3 = load(\"./model/grid_tree_3.joblib\")\n",
    "grid_tree_4 = load(\"./model/grid_tree_4.joblib\")\n",
    "grid_tree_5 = load(\"./model/grid_tree_5.joblib\")\n",
    "\n",
    "tree_1 = grid_tree_1.best_estimator_\n",
    "tree_2 = grid_tree_2.best_estimator_\n",
    "tree_3 = grid_tree_3.best_estimator_\n",
    "tree_4 = grid_tree_4.best_estimator_\n",
    "tree_5 = grid_tree_5.best_estimator_\n",
    "\n",
    "tree_l = [tree_1, tree_2, tree_3, tree_4, tree_5]\n",
    "\n",
    "# 逻辑回归模型组\n",
    "grid_lr_1 = load(\"./model/grid_lr_1.joblib\")\n",
    "grid_lr_2 = load(\"./model/grid_lr_2.joblib\")\n",
    "grid_lr_3 = load(\"./model/grid_lr_3.joblib\")\n",
    "grid_lr_4 = load(\"./model/grid_lr_4.joblib\")\n",
    "grid_lr_5 = load(\"./model/grid_lr_5.joblib\")\n",
    "\n",
    "lr_1 = grid_lr_1.best_estimator_\n",
    "lr_2 = grid_lr_2.best_estimator_\n",
    "lr_3 = grid_lr_3.best_estimator_\n",
    "lr_4 = grid_lr_4.best_estimator_\n",
    "lr_5 = grid_lr_5.best_estimator_\n",
    "\n",
    "lr_l = [lr_1, lr_2, lr_3, lr_4, lr_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval1_predict_proba_RF = pd.Series(\n",
    "    RF_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_RF = pd.Series(\n",
    "    RF_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_RF = pd.Series(\n",
    "    RF_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_RF = pd.Series(\n",
    "    RF_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_RF = pd.Series(\n",
    "    RF_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_RF = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_RF,\n",
    "        eval2_predict_proba_RF,\n",
    "        eval3_predict_proba_RF,\n",
    "        eval4_predict_proba_RF,\n",
    "        eval5_predict_proba_RF,\n",
    "    ]\n",
    ").sort_index()\n",
    "\n",
    "eval1_predict_proba_tree = pd.Series(\n",
    "    tree_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_tree = pd.Series(\n",
    "    tree_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_tree = pd.Series(\n",
    "    tree_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_tree = pd.Series(\n",
    "    tree_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_tree = pd.Series(\n",
    "    tree_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_tree = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_tree,\n",
    "        eval2_predict_proba_tree,\n",
    "        eval3_predict_proba_tree,\n",
    "        eval4_predict_proba_tree,\n",
    "        eval5_predict_proba_tree,\n",
    "    ]\n",
    ").sort_index()\n",
    "\n",
    "eval1_predict_proba_lr = pd.Series(\n",
    "    lr_l[0].predict_proba(X_eval1)[:, 1], index=X_eval1.index\n",
    ")\n",
    "eval2_predict_proba_lr = pd.Series(\n",
    "    lr_l[1].predict_proba(X_eval2)[:, 1], index=X_eval2.index\n",
    ")\n",
    "eval3_predict_proba_lr = pd.Series(\n",
    "    lr_l[2].predict_proba(X_eval3)[:, 1], index=X_eval3.index\n",
    ")\n",
    "eval4_predict_proba_lr = pd.Series(\n",
    "    lr_l[3].predict_proba(X_eval4)[:, 1], index=X_eval4.index\n",
    ")\n",
    "eval5_predict_proba_lr = pd.Series(\n",
    "    lr_l[4].predict_proba(X_eval5)[:, 1], index=X_eval5.index\n",
    ")\n",
    "\n",
    "eval_predict_proba_lr = pd.concat(\n",
    "    [\n",
    "        eval1_predict_proba_lr,\n",
    "        eval2_predict_proba_lr,\n",
    "        eval3_predict_proba_lr,\n",
    "        eval4_predict_proba_lr,\n",
    "        eval5_predict_proba_lr,\n",
    "    ]\n",
    ").sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict_proba_RF = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_RF.append(RF_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_RF = np.array(test_predict_proba_RF)\n",
    "test_predict_proba_RF = test_predict_proba_RF.mean(0)\n",
    "\n",
    "test_predict_proba_tree = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_tree.append(tree_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_tree = np.array(test_predict_proba_tree)\n",
    "test_predict_proba_tree = test_predict_proba_tree.mean(0)\n",
    "\n",
    "test_predict_proba_lr = []\n",
    "\n",
    "for i in range(5):\n",
    "    test_predict_proba_lr.append(lr_l[i].predict_proba(X_test_OE)[:, 1])\n",
    "\n",
    "test_predict_proba_lr = np.array(test_predict_proba_lr)\n",
    "test_predict_proba_lr = test_predict_proba_lr.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = load(\"./model/tree_model.joblib\")\n",
    "RF = load(\"./model/RF_0.joblib\")\n",
    "# logistic_search = load(\"./model/logistic_search.joblib\")\n",
    "\n",
    "lr = lr_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [(\"lr\", lr), (\"tree\", tree), (\"RF\", RF)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5392773151397705\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators, blending=True\n",
    ")\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train2-Accuracy: 0.824976, Test-Accuracy: 0.792164\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(\n",
    "    train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]\n",
    ")\n",
    "print(\"The results of LR-final:\")\n",
    "print(\n",
    "    \"Train2-Accuracy: %f, Test-Accuracy: %f\"\n",
    "    % (\n",
    "        lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]),\n",
    "        lr.score(test_predict_blending, y_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_space = {\"test_size\": hp.uniform(\"test_size\", 0.1, 0.5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_res(params, train=True):\n",
    "    test_size = params[\"test_size\"]\n",
    "    train_oof_blending, test_predict_blending = train_cross(\n",
    "        X_train_OE, y_train, X_test_OE, estimators, blending=True, test_size=test_size\n",
    "    )\n",
    "    lr = LogisticRegression().fit(\n",
    "        train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]\n",
    "    )\n",
    "    if train == True:\n",
    "        res = -lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1])\n",
    "    else:\n",
    "        res = (train_oof_blending, test_predict_blending)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_split_res(max_evals):\n",
    "    return fmin(\n",
    "        fn=split_res,\n",
    "        space=split_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        rstate=np.random.default_rng(11),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:23<00:00,  1.44s/trial, best loss: -0.8296007789678675]\n"
     ]
    }
   ],
   "source": [
    "best_split = param_split_res(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_size': 0.19433909363294544}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_oof_blending, test_predict_blending = split_res(best_split, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of LR-final:\n",
      "Train-oof-Accuracy: 0.829601, Test-Accuracy: 0.792164\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression().fit(\n",
    "    train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]\n",
    ")\n",
    "print(\"The results of LR-final:\")\n",
    "print(\n",
    "    \"Train-oof-Accuracy: %f, Test-Accuracy: %f\"\n",
    "    % (\n",
    "        lr.score(train_oof_blending.iloc[:, :-1], train_oof_blending.iloc[:, -1]),\n",
    "        lr.score(test_predict_blending, y_test),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一级学习器\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:38<00:00,  4.92s/trial, best loss: -0.789663125161959] \n",
      "100%|██████████| 20/20 [01:32<00:00,  4.62s/trial, best loss: -0.7726220091560854]\n",
      "100%|██████████| 20/20 [01:30<00:00,  4.52s/trial, best loss: -0.7658624859635484]\n",
      "100%|██████████| 20/20 [01:35<00:00,  4.79s/trial, best loss: -0.7934831994471796]\n",
      "100%|██████████| 20/20 [01:33<00:00,  4.66s/trial, best loss: -0.7923054331864904]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.35trial/s, best loss: -0.7993534594454521]\n",
      "100%|██████████| 1000/1000 [00:31<00:00, 31.91trial/s, best loss: -0.7973041375140365]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.74trial/s, best loss: -0.7975995508335492]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.91trial/s, best loss: -0.7987691111686965]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.50trial/s, best loss: -0.7990658201606633]\n",
      "100%|██████████| 500/500 [04:23<00:00,  1.90trial/s, best loss: -0.8064114191932278]\n",
      "100%|██████████| 500/500 [04:44<00:00,  1.76trial/s, best loss: -0.807874665284616] \n",
      "100%|██████████| 500/500 [04:22<00:00,  1.91trial/s, best loss: -0.8037682473870605]\n",
      "100%|██████████| 500/500 [04:15<00:00,  1.95trial/s, best loss: -0.8087565863349745]\n",
      "100%|██████████| 500/500 [04:03<00:00,  2.05trial/s, best loss: -0.804644985747603] \n"
     ]
    }
   ],
   "source": [
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE,\n",
    "    y_train,\n",
    "    X_test_OE,\n",
    "    estimators=estimators,\n",
    "    test_size=0.19433,\n",
    "    blending=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义元学习器搜索空间\n",
    "lr_final_param = [\n",
    "    {\n",
    "        \"thr\": np.arange(0.1, 1.1, 0.1).tolist(),\n",
    "        \"penalty\": [\"l1\"],\n",
    "        \"C\": np.arange(0.1, 1.1, 0.1).tolist(),\n",
    "        \"solver\": [\"saga\"],\n",
    "    },\n",
    "    {\n",
    "        \"thr\": np.arange(0.1, 1.1, 0.1).tolist(),\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"C\": np.arange(0.1, 1.1, 0.1).tolist(),\n",
    "        \"solver\": [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "tree_final_param = {\n",
    "    \"max_depth\": np.arange(2, 16, 1).tolist(),\n",
    "    \"min_samples_split\": np.arange(2, 5, 1).tolist(),\n",
    "    \"min_samples_leaf\": np.arange(1, 4, 1).tolist(),\n",
    "    \"max_leaf_nodes\": np.arange(6, 30, 1).tolist(),\n",
    "}\n",
    "\n",
    "param_space_l = [lr_final_param, tree_final_param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义元学习器列表\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行元学习器训练搜索\n",
    "best_res_final, best_test_predict_final = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8481012658227848"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_res_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853492333901193"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score((best_test_predict_final >= 0.5) * 1, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:44<00:00,  5.24s/trial, best loss: -0.7919527629849921]\n",
      "100%|██████████| 20/20 [01:54<00:00,  5.75s/trial, best loss: -0.7893204924268622]\n",
      "100%|██████████| 20/20 [01:47<00:00,  5.37s/trial, best loss: -0.7646012863960163]\n",
      "100%|██████████| 20/20 [01:37<00:00,  4.86s/trial, best loss: -0.796736288816654] \n",
      "100%|██████████| 20/20 [01:32<00:00,  4.60s/trial, best loss: -0.7956840030430874]\n",
      "100%|██████████| 1000/1000 [00:32<00:00, 30.67trial/s, best loss: -0.7958990939899024]\n",
      "100%|██████████| 1000/1000 [00:32<00:00, 30.97trial/s, best loss: -0.7861650183276853]\n",
      "100%|██████████| 1000/1000 [00:32<00:00, 30.84trial/s, best loss: -0.7887986721073379]\n",
      "100%|██████████| 1000/1000 [00:32<00:00, 30.60trial/s, best loss: -0.8006867694861333]\n",
      "100%|██████████| 1000/1000 [00:32<00:00, 30.48trial/s, best loss: -0.7996331004910436]\n",
      "100%|██████████| 500/500 [04:15<00:00,  1.96trial/s, best loss: -0.8129932913756137]\n",
      "100%|██████████| 500/500 [04:28<00:00,  1.86trial/s, best loss: -0.8024728542776126]\n",
      "100%|██████████| 500/500 [04:51<00:00,  1.72trial/s, best loss: -0.8019451552666158]\n",
      "100%|██████████| 500/500 [04:58<00:00,  1.68trial/s, best loss: -0.808051040874196] \n",
      "100%|██████████| 500/500 [04:53<00:00,  1.70trial/s, best loss: -0.8117300643198009]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators=estimators, test_size=0.1, blending=True\n",
    ")\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final1, best_test_predict_final1 = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:38<00:00,  4.93s/trial, best loss: -0.7914201183431953]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.30s/trial, best loss: -0.792603550295858] \n",
      "100%|██████████| 20/20 [01:33<00:00,  4.67s/trial, best loss: -0.7837278106508876]\n",
      "100%|██████████| 20/20 [01:36<00:00,  4.83s/trial, best loss: -0.7884615384615385]\n",
      "100%|██████████| 20/20 [01:34<00:00,  4.70s/trial, best loss: -0.7902366863905326]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 33.26trial/s, best loss: -0.7973372781065089]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.99trial/s, best loss: -0.8041420118343193]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.50trial/s, best loss: -0.7899408284023668]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.38trial/s, best loss: -0.8023668639053254]\n",
      "100%|██████████| 1000/1000 [00:30<00:00, 32.97trial/s, best loss: -0.7911242603550297]\n",
      "100%|██████████| 500/500 [04:45<00:00,  1.75trial/s, best loss: -0.8038461538461539]\n",
      "100%|██████████| 500/500 [04:45<00:00,  1.75trial/s, best loss: -0.808284023668639] \n",
      "100%|██████████| 500/500 [04:11<00:00,  1.99trial/s, best loss: -0.8026627218934911]\n",
      "100%|██████████| 500/500 [03:56<00:00,  2.12trial/s, best loss: -0.8103550295857989]\n",
      "100%|██████████| 500/500 [04:26<00:00,  1.88trial/s, best loss: -0.8044378698224852]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators=estimators, test_size=0.2, blending=True\n",
    ")\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final2, best_test_predict_final2 = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:28<00:00,  4.43s/trial, best loss: -0.788297434490328] \n",
      "100%|██████████| 20/20 [01:29<00:00,  4.50s/trial, best loss: -0.7855907303242328]\n",
      "100%|██████████| 20/20 [01:31<00:00,  4.57s/trial, best loss: -0.7829657703388668]\n",
      "100%|██████████| 20/20 [01:26<00:00,  4.34s/trial, best loss: -0.782290094663191]\n",
      "100%|██████████| 20/20 [01:48<00:00,  5.45s/trial, best loss: -0.789044564869438]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 34.73trial/s, best loss: -0.8038642703617324]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 35.08trial/s, best loss: -0.7994632322678008]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 34.91trial/s, best loss: -0.8015559976219876]\n",
      "100%|██████████| 1000/1000 [00:29<00:00, 34.26trial/s, best loss: -0.7866825581927104]\n",
      "100%|██████████| 1000/1000 [00:28<00:00, 34.58trial/s, best loss: -0.8123713815338182]\n",
      "100%|██████████| 500/500 [04:19<00:00,  1.93trial/s, best loss: -0.8079120364018841]\n",
      "100%|██████████| 500/500 [03:48<00:00,  2.19trial/s, best loss: -0.8079211826039238]\n",
      "100%|██████████| 500/500 [04:26<00:00,  1.88trial/s, best loss: -0.8076405085288334]\n",
      "100%|██████████| 500/500 [04:38<00:00,  1.80trial/s, best loss: -0.7988544381945397]\n",
      "100%|██████████| 500/500 [04:12<00:00,  1.98trial/s, best loss: -0.8130493437600036]\n"
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators=estimators, test_size=0.3, blending=True\n",
    ")\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final3, best_test_predict_final3 = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:17<00:00,  3.89s/trial, best loss: -0.7609467455621302]\n",
      "100%|██████████| 20/20 [01:22<00:00,  4.11s/trial, best loss: -0.7893491124260354]\n",
      "100%|██████████| 20/20 [01:21<00:00,  4.08s/trial, best loss: -0.778698224852071] \n",
      "100%|██████████| 20/20 [01:20<00:00,  4.01s/trial, best loss: -0.7834319526627219]\n",
      "100%|██████████| 20/20 [01:25<00:00,  4.28s/trial, best loss: -0.7886447995775676]\n",
      "100%|██████████| 1000/1000 [00:27<00:00, 36.79trial/s, best loss: -0.7893491124260356]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 37.12trial/s, best loss: -0.7964497041420119]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 38.24trial/s, best loss: -0.791715976331361]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 37.95trial/s, best loss: -0.7952662721893491]\n",
      "100%|██████████| 1000/1000 [00:26<00:00, 37.97trial/s, best loss: -0.7981099256084113]\n",
      "100%|██████████| 500/500 [03:47<00:00,  2.20trial/s, best loss: -0.8039447731755424]\n",
      "100%|██████████| 500/500 [04:07<00:00,  2.02trial/s, best loss: -0.808284023668639] \n",
      " 27%|██▋       | 133/500 [01:01<02:51,  2.15trial/s, best loss: -0.8027613412228798]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m RF_hyper \u001b[39m=\u001b[39m RF_cascade(RF_params_space)\n\u001b[0;32m      6\u001b[0m estimators \u001b[39m=\u001b[39m [(\u001b[39m\"\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m\"\u001b[39m, lr_hyper), (\u001b[39m\"\u001b[39m\u001b[39mtree\u001b[39m\u001b[39m\"\u001b[39m, tree_hyper), (\u001b[39m\"\u001b[39m\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m, RF_hyper)]\n\u001b[1;32m----> 8\u001b[0m train_oof_blending, test_predict_blending \u001b[39m=\u001b[39m train_cross(\n\u001b[0;32m      9\u001b[0m     X_train_OE, y_train, X_test_OE, estimators\u001b[39m=\u001b[39;49mestimators, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.4\u001b[39;49m, blending\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[39m# 元学习器训练与优化\u001b[39;00m\n\u001b[0;32m     13\u001b[0m lr \u001b[39m=\u001b[39m logit_threshold()\n",
      "File \u001b[1;32mf:\\study\\python\\code6\\More advanced machine learning\\03_模型融合\\manual_ensemble.py:236\u001b[0m, in \u001b[0;36mtrain_cross\u001b[1;34m(X_train, y_train, X_test, estimators, test_size, n_splits, random_state, blending)\u001b[0m\n\u001b[0;32m    234\u001b[0m X_train_part \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mloc[train_part_index]\n\u001b[0;32m    235\u001b[0m y_train_part \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mloc[train_part_index]\n\u001b[1;32m--> 236\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train_part, y_train_part)\n\u001b[0;32m    237\u001b[0m \u001b[39mif\u001b[39;00m blending \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[39m# 在留出集上进行预测并求均值\u001b[39;00m\n\u001b[0;32m    239\u001b[0m     train_oof[oof_colName] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict_proba(X1)[:, \u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m n_splits\n",
      "File \u001b[1;32mf:\\study\\python\\code6\\More advanced machine learning\\03_模型融合\\manual_ensemble.py:523\u001b[0m, in \u001b[0;36mRF_cascade.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    514\u001b[0m     params_best \u001b[39m=\u001b[39m fmin(\n\u001b[0;32m    515\u001b[0m         fn\u001b[39m=\u001b[39mhyperopt_RF,\n\u001b[0;32m    516\u001b[0m         space\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mRF_params_space,\n\u001b[0;32m    517\u001b[0m         algo\u001b[39m=\u001b[39mtpe\u001b[39m.\u001b[39msuggest,\n\u001b[0;32m    518\u001b[0m         max_evals\u001b[39m=\u001b[39mmax_evals,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m     \u001b[39mreturn\u001b[39;00m params_best\n\u001b[1;32m--> 523\u001b[0m RF_params_best \u001b[39m=\u001b[39m param_hyperopt_RF(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals)\n\u001b[0;32m    524\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclf \u001b[39m=\u001b[39m hyperopt_RF(RF_params_best, train\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m    525\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mf:\\study\\python\\code6\\More advanced machine learning\\03_模型融合\\manual_ensemble.py:514\u001b[0m, in \u001b[0;36mRF_cascade.fit.<locals>.param_hyperopt_RF\u001b[1;34m(max_evals)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparam_hyperopt_RF\u001b[39m(max_evals):\n\u001b[1;32m--> 514\u001b[0m     params_best \u001b[39m=\u001b[39m fmin(\n\u001b[0;32m    515\u001b[0m         fn\u001b[39m=\u001b[39;49mhyperopt_RF,\n\u001b[0;32m    516\u001b[0m         space\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mRF_params_space,\n\u001b[0;32m    517\u001b[0m         algo\u001b[39m=\u001b[39;49mtpe\u001b[39m.\u001b[39;49msuggest,\n\u001b[0;32m    518\u001b[0m         max_evals\u001b[39m=\u001b[39;49mmax_evals,\n\u001b[0;32m    519\u001b[0m     )\n\u001b[0;32m    521\u001b[0m     \u001b[39mreturn\u001b[39;00m params_best\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\hyperopt\\fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    583\u001b[0m rval\u001b[39m.\u001b[39mcatch_eval_exceptions \u001b[39m=\u001b[39m catch_eval_exceptions\n\u001b[0;32m    585\u001b[0m \u001b[39m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m rval\u001b[39m.\u001b[39;49mexhaust()\n\u001b[0;32m    588\u001b[0m \u001b[39mif\u001b[39;00m return_argmin:\n\u001b[0;32m    589\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(trials\u001b[39m.\u001b[39mtrials) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\hyperopt\\fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexhaust\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    363\u001b[0m     n_done \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials)\n\u001b[1;32m--> 364\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_evals \u001b[39m-\u001b[39;49m n_done, block_until_done\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49masynchronous)\n\u001b[0;32m    365\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\hyperopt\\fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    297\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpoll_interval_secs)\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mserial_evaluate()\n\u001b[0;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials\u001b[39m.\u001b[39mrefresh()\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials_save_file \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\hyperopt\\fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m ctrl \u001b[39m=\u001b[39m base\u001b[39m.\u001b[39mCtrl(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrials, current_trial\u001b[39m=\u001b[39mtrial)\n\u001b[0;32m    177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mevaluate(spec, ctrl)\n\u001b[0;32m    179\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    180\u001b[0m     logger\u001b[39m.\u001b[39merror(\u001b[39m\"\u001b[39m\u001b[39mjob exception: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mstr\u001b[39m(e))\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\hyperopt\\base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    884\u001b[0m     \u001b[39m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[0;32m    885\u001b[0m     \u001b[39m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[0;32m    886\u001b[0m     \u001b[39m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     pyll_rval \u001b[39m=\u001b[39m pyll\u001b[39m.\u001b[39mrec_eval(\n\u001b[0;32m    888\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpr,\n\u001b[0;32m    889\u001b[0m         memo\u001b[39m=\u001b[39mmemo,\n\u001b[0;32m    890\u001b[0m         print_node_on_error\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[0;32m    891\u001b[0m     )\n\u001b[1;32m--> 892\u001b[0m     rval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(pyll_rval)\n\u001b[0;32m    894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(rval, (\u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39mnumber)):\n\u001b[0;32m    895\u001b[0m     dict_rval \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mfloat\u001b[39m(rval), \u001b[39m\"\u001b[39m\u001b[39mstatus\u001b[39m\u001b[39m\"\u001b[39m: STATUS_OK}\n",
      "File \u001b[1;32mf:\\study\\python\\code6\\More advanced machine learning\\03_模型融合\\manual_ensemble.py:507\u001b[0m, in \u001b[0;36mRF_cascade.fit.<locals>.hyperopt_RF\u001b[1;34m(params, train)\u001b[0m\n\u001b[0;32m    497\u001b[0m RF \u001b[39m=\u001b[39m RandomForestClassifier(\n\u001b[0;32m    498\u001b[0m     min_samples_leaf\u001b[39m=\u001b[39mmin_samples_leaf,\n\u001b[0;32m    499\u001b[0m     min_samples_split\u001b[39m=\u001b[39mmin_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m     random_state\u001b[39m=\u001b[39m\u001b[39m9\u001b[39m,\n\u001b[0;32m    505\u001b[0m )\n\u001b[0;32m    506\u001b[0m \u001b[39mif\u001b[39;00m train \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 507\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mcross_val_score(RF, X, y)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    508\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    509\u001b[0m     res \u001b[39m=\u001b[39m RF\u001b[39m.\u001b[39mfit(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:465\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[1;32m--> 465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    466\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:466\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    461\u001b[0m     \u001b[39m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[39m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m     random_state\u001b[39m.\u001b[39mrandint(MAX_INT, size\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_))\n\u001b[0;32m    465\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_estimator(append\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, random_state\u001b[39m=\u001b[39;49mrandom_state)\n\u001b[0;32m    467\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    468\u001b[0m ]\n\u001b[0;32m    470\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    477\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs,\n\u001b[0;32m    478\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[39mfor\u001b[39;00m i, t \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trees)\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\ensemble\\_base.py:163\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_estimator\u001b[39m(\u001b[39mself\u001b[39m, append\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    158\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Make and configure a copy of the `base_estimator_` attribute.\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \n\u001b[0;32m    160\u001b[0m \u001b[39m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39m    sub-estimators.\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 163\u001b[0m     estimator \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_estimator_)\n\u001b[0;32m    164\u001b[0m     estimator\u001b[39m.\u001b[39mset_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{p: \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, p) \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimator_params})\n\u001b[0;32m    166\u001b[0m     \u001b[39m# TODO: Remove in v1.2\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# criterion \"mse\" and \"mae\" would cause warnings in every call to\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# DecisionTreeRegressor.fit(..)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:85\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     78\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m (type \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m): \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mit does not seem to be a scikit-learn \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mestimator as it does not implement a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m'\u001b[39m\u001b[39m method.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mrepr\u001b[39m(estimator), \u001b[39mtype\u001b[39m(estimator))\n\u001b[0;32m     82\u001b[0m             )\n\u001b[0;32m     84\u001b[0m klass \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\n\u001b[1;32m---> 85\u001b[0m new_object_params \u001b[39m=\u001b[39m estimator\u001b[39m.\u001b[39;49mget_params(deep\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:210\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39mGet parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    209\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m--> 210\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_param_names():\n\u001b[0;32m    211\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, key)\n\u001b[0;32m    212\u001b[0m     \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mget_params\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:175\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m    173\u001b[0m \u001b[39m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[39m# to represent\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m init_signature \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49msignature(init)\n\u001b[0;32m    176\u001b[0m \u001b[39m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[0;32m    177\u001b[0m parameters \u001b[39m=\u001b[39m [\n\u001b[0;32m    178\u001b[0m     p\n\u001b[0;32m    179\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m init_signature\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mvalues()\n\u001b[0;32m    180\u001b[0m     \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mname \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mself\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mkind \u001b[39m!=\u001b[39m p\u001b[39m.\u001b[39mVAR_KEYWORD\n\u001b[0;32m    181\u001b[0m ]\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\inspect.py:3113\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   3111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msignature\u001b[39m(obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   3112\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3113\u001b[0m     \u001b[39mreturn\u001b[39;00m Signature\u001b[39m.\u001b[39;49mfrom_callable(obj, follow_wrapped\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\inspect.py:2862\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[0;32m   2859\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m   2860\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_callable\u001b[39m(\u001b[39mcls\u001b[39m, obj, \u001b[39m*\u001b[39m, follow_wrapped\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   2861\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 2862\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m,\n\u001b[0;32m   2863\u001b[0m                                     follow_wrapper_chains\u001b[39m=\u001b[39;49mfollow_wrapped)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\inspect.py:2325\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[0;32m   2320\u001b[0m             \u001b[39mreturn\u001b[39;00m sig\u001b[39m.\u001b[39mreplace(parameters\u001b[39m=\u001b[39mnew_params)\n\u001b[0;32m   2322\u001b[0m \u001b[39mif\u001b[39;00m isfunction(obj) \u001b[39mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[0;32m   2323\u001b[0m     \u001b[39m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m     \u001b[39m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m-> 2325\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[0;32m   2326\u001b[0m                                     skip_bound_arg\u001b[39m=\u001b[39;49mskip_bound_arg)\n\u001b[0;32m   2328\u001b[0m \u001b[39mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m   2329\u001b[0m     \u001b[39mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[0;32m   2330\u001b[0m                                    skip_bound_arg\u001b[39m=\u001b[39mskip_bound_arg)\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\inspect.py:2225\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg)\u001b[0m\n\u001b[0;32m   2222\u001b[0m         default \u001b[39m=\u001b[39m kwdefaults\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[0;32m   2224\u001b[0m     annotation \u001b[39m=\u001b[39m annotations\u001b[39m.\u001b[39mget(name, _empty)\n\u001b[1;32m-> 2225\u001b[0m     parameters\u001b[39m.\u001b[39mappend(Parameter(name, annotation\u001b[39m=\u001b[39;49mannotation,\n\u001b[0;32m   2226\u001b[0m                                 kind\u001b[39m=\u001b[39;49m_KEYWORD_ONLY,\n\u001b[0;32m   2227\u001b[0m                                 default\u001b[39m=\u001b[39;49mdefault))\n\u001b[0;32m   2228\u001b[0m \u001b[39m# **kwargs\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m \u001b[39mif\u001b[39;00m func_code\u001b[39m.\u001b[39mco_flags \u001b[39m&\u001b[39m CO_VARKEYWORDS:\n",
      "File \u001b[1;32md:\\IDE\\Anaconda\\envs\\fproject\\lib\\inspect.py:2500\u001b[0m, in \u001b[0;36mParameter.__init__\u001b[1;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[0;32m   2498\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, kind, \u001b[39m*\u001b[39m, default\u001b[39m=\u001b[39m_empty, annotation\u001b[39m=\u001b[39m_empty):\n\u001b[0;32m   2499\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2500\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kind \u001b[39m=\u001b[39m _ParameterKind(kind)\n\u001b[0;32m   2501\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2502\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalue \u001b[39m\u001b[39m{\u001b[39;00mkind\u001b[39m!r}\u001b[39;00m\u001b[39m is not a valid Parameter.kind\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators=estimators, test_size=0.4, blending=True\n",
    ")\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final4, best_test_predict_final4 = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一级学习器交叉训练\n",
    "lr_hyper = lr_cascade(lr_params_space)\n",
    "tree_hyper = tree_cascade(tree_params_space)\n",
    "RF_hyper = RF_cascade(RF_params_space)\n",
    "\n",
    "estimators = [(\"lr\", lr_hyper), (\"tree\", tree_hyper), (\"rf\", RF_hyper)]\n",
    "\n",
    "train_oof_blending, test_predict_blending = train_cross(\n",
    "    X_train_OE, y_train, X_test_OE, estimators=estimators, test_size=0.5, blending=True\n",
    ")\n",
    "\n",
    "# 元学习器训练与优化\n",
    "lr = logit_threshold()\n",
    "tree = DecisionTreeClassifier()\n",
    "final_model_l = [lr, tree]\n",
    "\n",
    "best_res_final5, best_test_predict_final5 = final_model_opt(\n",
    "    final_model_l,\n",
    "    param_space_l,\n",
    "    train_oof_blending.iloc[:, :-1],\n",
    "    train_oof_blending.iloc[:, -1],\n",
    "    test_predict_blending,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Blending_res = pd.DataFrame(\n",
    "    {\n",
    "        \"res1\": best_test_predict_final1,\n",
    "        \"res2\": best_test_predict_final2,\n",
    "        \"res3\": best_test_predict_final3,\n",
    "        \"res4\": best_test_predict_final4,\n",
    "        \"res5\": best_test_predict_final5,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 写入本地\n",
    "Blending_res.to_csv(\"Blending_res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Blending_res.mean(axis=1) > 0.5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(\n",
    "    [\n",
    "        best_res_final1,\n",
    "        best_res_final2,\n",
    "        best_res_final3,\n",
    "        best_res_final4,\n",
    "        best_res_final5,\n",
    "    ],\n",
    "    index=[\n",
    "        \"best_res_final1\",\n",
    "        \"best_res_final2\",\n",
    "        \"best_res_final3\",\n",
    "        \"best_res_final4\",\n",
    "        \"best_res_final5\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加权平均融合\n",
    "Blending_res1 = (\n",
    "    (Blending_res[\"res1\"] * 4)\n",
    "    + (Blending_res[\"res2\"] * 5)\n",
    "    + (Blending_res[\"res3\"] * 2)\n",
    "    + (Blending_res[\"res4\"] * 3)\n",
    "    + (Blending_res[\"res5\"] * 1)\n",
    ") / 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(Blending_res1 > 0.5, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
